{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paul/opt/miniconda3/envs/qtc/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x1_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/paul/Desktop/JHElab/Project/Neural ODE/Lab-04-1 Multivariable Linear regression.ipynb 셀 1\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/paul/Desktop/JHElab/Project/Neural%20ODE/Lab-04-1%20Multivariable%20Linear%20regression.ipynb#W0sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m y_train \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mFloatTensor([[\u001b[39m152\u001b[39m],[\u001b[39m185\u001b[39m],[\u001b[39m180\u001b[39m],[\u001b[39m196\u001b[39m],[\u001b[39m142\u001b[39m]])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/paul/Desktop/JHElab/Project/Neural%20ODE/Lab-04-1%20Multivariable%20Linear%20regression.ipynb#W0sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# H(x) 계산\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/paul/Desktop/JHElab/Project/Neural%20ODE/Lab-04-1%20Multivariable%20Linear%20regression.ipynb#W0sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m hypothesis \u001b[39m=\u001b[39m x1_train \u001b[39m*\u001b[39m w1 \u001b[39m+\u001b[39m x2_train \u001b[39m*\u001b[39m w2 \u001b[39m+\u001b[39m x3_train \u001b[39m*\u001b[39m w3 \u001b[39m+\u001b[39m b \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/paul/Desktop/JHElab/Project/Neural%20ODE/Lab-04-1%20Multivariable%20Linear%20regression.ipynb#W0sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# x_train이 1000개 라면 너무 복잡함 -> 더 간결하고, x의 길이가 바뀌어도 코드를 바꿀 필요가 없고, 속도도 더 빠른 matmul()로 계산!\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/paul/Desktop/JHElab/Project/Neural%20ODE/Lab-04-1%20Multivariable%20Linear%20regression.ipynb#W0sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m hypothesis \u001b[39m=\u001b[39m x_train\u001b[39m.\u001b[39mmatmul(W) \u001b[39m+\u001b[39m b \u001b[39m# or .mm or @\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x1_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Lab-04-1 Multivariable Linear regression\n",
    "import torch\n",
    "x_train = torch.FloatTensor([[73,80,75],[93,88,93],[89,91,90],[96,98,100],[73,66,70]])\n",
    "y_train = torch.FloatTensor([[152],[185],[180],[196],[142]])\n",
    "\n",
    "# H(x) 계산 \n",
    "# hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 + b \n",
    "# x_train이 1000개 라면 너무 복잡함 -> 더 간결하고, x의 길이가 바뀌어도 코드를 바꿀 필요가 없고, 속도도 더 빠른 matmul()로 계산!\n",
    "hypothesis = x_train.matmul(W) + b # or .mm or @\n",
    "cost = torch.mean*((hypothesis - y_train)**2) # 기존 simple linear regression과 동일한 공식!\n",
    "# W : = W -a*grad(W)\n",
    "# optimizer설정\n",
    "optimizer = torch.optim.SGD([W, b], lr=1e-5)\n",
    "\n",
    "# optimizer 사용법\n",
    "optimizer.zero_grad()\n",
    "cost.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Code with torch.optim(1)\n",
    "\n",
    "# 데이터\n",
    "x_train = torch.FloatTensor([[73, 80, 75], [93,88, 93], [89, 91, 90], [96, 98, 100], [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152],[185],[180],[196],[142]])\n",
    "\n",
    "# 모델 초기화\n",
    "W = torch.zeros((3, 1), requires_grad = True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = torch.optim.SGD([W, b], lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n",
      "Epoch    1/20 hypothesis: tensor([67.2578, 80.8397, 79.6523, 86.7394, 61.6605]) Cost: 9298.520508\n",
      "Epoch    2/20 hypothesis: tensor([104.9128, 126.0990, 124.2466, 135.3015,  96.1821]) Cost: 2915.712402\n",
      "Epoch    3/20 hypothesis: tensor([125.9942, 151.4381, 149.2133, 162.4896, 115.5097]) Cost: 915.040649\n",
      "Epoch    4/20 hypothesis: tensor([137.7967, 165.6247, 163.1911, 177.7112, 126.3307]) Cost: 287.936157\n",
      "Epoch    5/20 hypothesis: tensor([144.4044, 173.5674, 171.0168, 186.2332, 132.3891]) Cost: 91.371010\n",
      "Epoch    6/20 hypothesis: tensor([148.1035, 178.0143, 175.3980, 191.0042, 135.7812]) Cost: 29.758249\n",
      "Epoch    7/20 hypothesis: tensor([150.1744, 180.5042, 177.8509, 193.6753, 137.6805]) Cost: 10.445281\n",
      "Epoch    8/20 hypothesis: tensor([151.3336, 181.8983, 179.2240, 195.1707, 138.7440]) Cost: 4.391237\n",
      "Epoch    9/20 hypothesis: tensor([151.9824, 182.6789, 179.9928, 196.0079, 139.3396]) Cost: 2.493121\n",
      "Epoch   10/20 hypothesis: tensor([152.3454, 183.1161, 180.4231, 196.4765, 139.6732]) Cost: 1.897688\n",
      "Epoch   11/20 hypothesis: tensor([152.5485, 183.3609, 180.6640, 196.7389, 139.8602]) Cost: 1.710555\n",
      "Epoch   12/20 hypothesis: tensor([152.6620, 183.4982, 180.7988, 196.8857, 139.9651]) Cost: 1.651412\n",
      "Epoch   13/20 hypothesis: tensor([152.7253, 183.5752, 180.8742, 196.9678, 140.0240]) Cost: 1.632369\n",
      "Epoch   14/20 hypothesis: tensor([152.7606, 183.6184, 180.9164, 197.0138, 140.0571]) Cost: 1.625924\n",
      "Epoch   15/20 hypothesis: tensor([152.7802, 183.6427, 180.9399, 197.0395, 140.0759]) Cost: 1.623420\n",
      "Epoch   16/20 hypothesis: tensor([152.7909, 183.6565, 180.9530, 197.0538, 140.0865]) Cost: 1.622141\n",
      "Epoch   17/20 hypothesis: tensor([152.7968, 183.6643, 180.9603, 197.0618, 140.0927]) Cost: 1.621262\n",
      "Epoch   18/20 hypothesis: tensor([152.7999, 183.6688, 180.9644, 197.0661, 140.0963]) Cost: 1.620501\n",
      "Epoch   19/20 hypothesis: tensor([152.8014, 183.6715, 180.9665, 197.0686, 140.0985]) Cost: 1.619764\n",
      "Epoch   20/20 hypothesis: tensor([152.8020, 183.6731, 180.9677, 197.0699, 140.0999]) Cost: 1.619046\n"
     ]
    }
   ],
   "source": [
    "# Full Code with torch.optim(2)\n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    # H(x) 계산 \n",
    "    hypothesis = x_train.matmul(W) + b # or .mm or @\n",
    "    \n",
    "    # cost 계산 \n",
    "    cost = torch.mean((hypothesis-y_train)**2)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print('Epoch {:4d}/{} hypothesis: {} Cost: {:.6f}'.format(epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item() ))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultivariateLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 1) # 입력차원 3, 출력차원 1\n",
    "        # Hypothesis 계산은 forward()에서!\n",
    "        # Gradient 계산은 PyTorch가 알아서 해준다 backward()\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "model = MultivariateLinearRegressionModel()\n",
    "prediction = model(x_train) \n",
    "\n",
    "# cost 계산 \n",
    "cost = F.mse_loss(prediction, y_train) # torch.nn.functional에서 제공하는 Loss function 사용 / 쉽게 다른 Loss와 교체 가능!(l1_loss, smooth_l1_loss 등)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Hypothesis: tensor([50.3983, 67.0624, 62.6675, 71.7771, 52.2501]), Cost: 12297.095703\n",
      "Epoch    1/20 Hypothesis: tensor([ 93.3019, 118.6690, 113.4956, 125.2913,  91.6184]), Cost: 3961.224609\n",
      "Epoch    2/20 Hypothesis: tensor([117.6486, 147.9542, 142.3390, 155.6588, 113.9586]), Cost: 1276.897827\n",
      "Epoch    3/20 Hypothesis: tensor([131.4648, 164.5725, 158.7069, 172.8913, 126.6359]), Cost: 412.488281\n",
      "Epoch    4/20 Hypothesis: tensor([139.3053, 174.0028, 167.9953, 182.6700, 133.8297]), Cost: 134.129318\n",
      "Epoch    5/20 Hypothesis: tensor([143.7548, 179.3542, 173.2663, 188.2190, 137.9119]), Cost: 44.491505\n",
      "Epoch    6/20 Hypothesis: tensor([146.2800, 182.3908, 176.2575, 191.3677, 140.2283]), Cost: 15.625875\n",
      "Epoch    7/20 Hypothesis: tensor([147.7132, 184.1140, 177.9551, 193.1544, 141.5426]), Cost: 6.329994\n",
      "Epoch    8/20 Hypothesis: tensor([148.5268, 185.0918, 178.9185, 194.1681, 142.2884]), Cost: 3.336112\n",
      "Epoch    9/20 Hypothesis: tensor([148.9886, 185.6466, 179.4653, 194.7431, 142.7114]), Cost: 2.371609\n",
      "Epoch   10/20 Hypothesis: tensor([149.2510, 185.9613, 179.7758, 195.0694, 142.9514]), Cost: 2.060564\n",
      "Epoch   11/20 Hypothesis: tensor([149.4001, 186.1399, 179.9521, 195.2543, 143.0874]), Cost: 1.959947\n",
      "Epoch   12/20 Hypothesis: tensor([149.4850, 186.2412, 180.0522, 195.3591, 143.1645]), Cost: 1.927109\n",
      "Epoch   13/20 Hypothesis: tensor([149.5333, 186.2986, 180.1091, 195.4184, 143.2081]), Cost: 1.916093\n",
      "Epoch   14/20 Hypothesis: tensor([149.5610, 186.3311, 180.1416, 195.4519, 143.2328]), Cost: 1.912108\n",
      "Epoch   15/20 Hypothesis: tensor([149.5770, 186.3494, 180.1601, 195.4708, 143.2466]), Cost: 1.910364\n",
      "Epoch   16/20 Hypothesis: tensor([149.5863, 186.3598, 180.1707, 195.4813, 143.2544]), Cost: 1.909390\n",
      "Epoch   17/20 Hypothesis: tensor([149.5918, 186.3657, 180.1769, 195.4872, 143.2587]), Cost: 1.908641\n",
      "Epoch   18/20 Hypothesis: tensor([149.5951, 186.3689, 180.1805, 195.4903, 143.2610]), Cost: 1.907961\n",
      "Epoch   19/20 Hypothesis: tensor([149.5973, 186.3707, 180.1827, 195.4919, 143.2622]), Cost: 1.907288\n",
      "Epoch   20/20 Hypothesis: tensor([149.5987, 186.3716, 180.1840, 195.4927, 143.2627]), Cost: 1.906644\n"
     ]
    }
   ],
   "source": [
    "model=MultivariateLinearRegressionModel()\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=1e-5)\n",
    "nb_epochs=20\n",
    "for epoch in range(nb_epochs+1):\n",
    "    Hypothesis=model(x_train)\n",
    "    cost=F.mse_loss(Hypothesis,y_train)\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    print('Epoch {:4d}/{} Hypothesis: {}, Cost: {:.6f}'.format(epoch,nb_epochs,Hypothesis.squeeze().detach(),cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1\ty_pred = tensor([-37.5671, -41.9411, -42.9742, -43.3251, -30.8874])\tLoss = 44864.3984375\n",
      "Epoch : 2\ty_pred = tensor([44.3844, 56.6366, 54.1149, 58.8971, 44.3133])\tLoss = 14449.052734375\n",
      "Epoch : 3\ty_pred = tensor([ 90.8891, 112.5763, 109.2099, 116.9052,  86.9874])\tLoss = 4654.6708984375\n",
      "Epoch : 4\ty_pred = tensor([117.2789, 144.3204, 140.4744, 149.8232, 111.2036])\tLoss = 1500.6734619140625\n",
      "Epoch : 5\ty_pred = tensor([132.2541, 162.3343, 158.2160, 168.5033, 124.9456])\tLoss = 485.01922607421875\n",
      "Epoch : 6\ty_pred = tensor([140.7518, 172.5566, 168.2837, 179.1039, 132.7438])\tLoss = 157.95677185058594\n",
      "Epoch : 7\ty_pred = tensor([145.5739, 178.3575, 173.9967, 185.1196, 137.1691])\tLoss = 52.63556671142578\n",
      "Epoch : 8\ty_pred = tensor([148.3100, 181.6494, 177.2385, 188.5336, 139.6803])\tLoss = 18.719371795654297\n",
      "Epoch : 9\ty_pred = tensor([149.8625, 183.5175, 179.0780, 190.4711, 141.1054])\tLoss = 7.797204494476318\n",
      "Epoch : 10\ty_pred = tensor([150.7433, 184.5776, 180.1218, 191.5708, 141.9141])\tLoss = 4.279603004455566\n",
      "Epoch : 11\ty_pred = tensor([151.2429, 185.1792, 180.7140, 192.1950, 142.3731])\tLoss = 3.1464459896087646\n",
      "Epoch : 12\ty_pred = tensor([151.5262, 185.5206, 181.0499, 192.5494, 142.6336])\tLoss = 2.781149387359619\n",
      "Epoch : 13\ty_pred = tensor([151.6868, 185.7144, 181.2404, 192.7508, 142.7814])\tLoss = 2.66306734085083\n",
      "Epoch : 14\ty_pred = tensor([151.7778, 185.8244, 181.3484, 192.8652, 142.8654])\tLoss = 2.624588966369629\n",
      "Epoch : 15\ty_pred = tensor([151.8292, 185.8869, 181.4096, 192.9304, 142.9130])\tLoss = 2.6117987632751465\n",
      "Epoch : 16\ty_pred = tensor([151.8581, 185.9223, 181.4443, 192.9676, 142.9401])\tLoss = 2.607236385345459\n",
      "Epoch : 17\ty_pred = tensor([151.8744, 185.9425, 181.4638, 192.9888, 142.9555])\tLoss = 2.605360269546509\n",
      "Epoch : 18\ty_pred = tensor([151.8834, 185.9540, 181.4748, 193.0011, 142.9643])\tLoss = 2.604330062866211\n",
      "Epoch : 19\ty_pred = tensor([151.8883, 185.9605, 181.4809, 193.0083, 142.9693])\tLoss = 2.6035757064819336\n",
      "Epoch : 20\ty_pred = tensor([151.8909, 185.9643, 181.4842, 193.0126, 142.9722])\tLoss = 2.6029019355773926\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "x_train = Variable(torch.FloatTensor([[73, 80, 75],\n",
    "                                      [93, 88, 93],\n",
    "                                      [89, 91, 90],\n",
    "                                      [96, 88, 100],\n",
    "                                      [73, 66, 70]]))\n",
    "y_train = Variable(torch.FloatTensor([[152],\n",
    "                                      [185],\n",
    "                                      [180],\n",
    "                                      [196],\n",
    "                                      [142]]))\n",
    "\n",
    "class MultivariateLinearRegressionModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultivariateLinearRegressionModel, self).__init__()\n",
    "        self.linear = torch.nn.Linear(3, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "\n",
    "model = MultivariateLinearRegressionModel()\n",
    "\n",
    "criterion = torch.nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)\n",
    "\n",
    "epochs = 20\n",
    "for epoch in range(1, epochs+1):\n",
    "    y_pred = model(x_train)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch : {epoch}\\ty_pred = {y_pred.squeeze().detach()}\\tLoss = {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qtc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3e34661f62bd8e945851969cf164be387ceff78bd554b7a7643e0a350d74b779"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
