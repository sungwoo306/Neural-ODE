{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paul/opt/miniconda3/envs/qtc/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'W' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/paul/Desktop/JHElab/Project/Neural ODE/Lab-04-1 Multivariable Linear regression.ipynb 셀 1\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/paul/Desktop/JHElab/Project/Neural%20ODE/Lab-04-1%20Multivariable%20Linear%20regression.ipynb#W0sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m y_train \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mFloatTensor([[\u001b[39m152\u001b[39m],[\u001b[39m185\u001b[39m],[\u001b[39m180\u001b[39m],[\u001b[39m196\u001b[39m],[\u001b[39m142\u001b[39m]])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/paul/Desktop/JHElab/Project/Neural%20ODE/Lab-04-1%20Multivariable%20Linear%20regression.ipynb#W0sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# H(x) 계산 \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/paul/Desktop/JHElab/Project/Neural%20ODE/Lab-04-1%20Multivariable%20Linear%20regression.ipynb#W0sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 + b \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/paul/Desktop/JHElab/Project/Neural%20ODE/Lab-04-1%20Multivariable%20Linear%20regression.ipynb#W0sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# x_train이 1000개 라면 너무 복잡함 -> 더 간결하고, x의 길이가 바뀌어도 코드를 바꿀 필요가 없고, 속도도 더 빠른 matmul()로 계산!\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/paul/Desktop/JHElab/Project/Neural%20ODE/Lab-04-1%20Multivariable%20Linear%20regression.ipynb#W0sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m hypothesis \u001b[39m=\u001b[39m x_train\u001b[39m.\u001b[39mmatmul(W) \u001b[39m+\u001b[39m b \u001b[39m# or .mm or @\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/paul/Desktop/JHElab/Project/Neural%20ODE/Lab-04-1%20Multivariable%20Linear%20regression.ipynb#W0sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m cost \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean\u001b[39m*\u001b[39m((hypothesis \u001b[39m-\u001b[39m y_train)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m) \u001b[39m# 기존 simple linear regression과 동일한 공식!\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/paul/Desktop/JHElab/Project/Neural%20ODE/Lab-04-1%20Multivariable%20Linear%20regression.ipynb#W0sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# W : = W -a*grad(W)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/paul/Desktop/JHElab/Project/Neural%20ODE/Lab-04-1%20Multivariable%20Linear%20regression.ipynb#W0sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# optimizer설정\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'W' is not defined"
     ]
    }
   ],
   "source": [
    "# Lab-04-1 Multivariable Linear regression\n",
    "import torch\n",
    "x_train = torch.FloatTensor([[73,80,75],[93,88,93],[89,91,90],[96,98,100],[73,66,70]])\n",
    "y_train = torch.FloatTensor([[152],[185],[180],[196],[142]])\n",
    "\n",
    "# H(x) 계산 \n",
    "# hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 + b \n",
    "# x_train이 1000개 라면 너무 복잡함 -> 더 간결하고, x의 길이가 바뀌어도 코드를 바꿀 필요가 없고, 속도도 더 빠른 matmul()로 계산!\n",
    "hypothesis = x_train.matmul(W) + b # or .mm or @\n",
    "cost = torch.mean*((hypothesis - y_train)**2) # 기존 simple linear regression과 동일한 공식!\n",
    "# W : = W -a*grad(W)\n",
    "# optimizer설정\n",
    "optimizer = torch.optim.SGD([W, b], lr=1e-5)\n",
    "\n",
    "# optimizer 사용법\n",
    "optimizer.zero_grad()\n",
    "cost.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Code with torch.optim(1)\n",
    "\n",
    "# 데이터\n",
    "x_train = torch.FloatTensor([[73, 80, 75], [93,88, 93], [89, 91, 90], [96, 98, 100], [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152],[185],[180],[196],[142]])\n",
    "\n",
    "# 모델 초기화\n",
    "W = torch.zeros((3, 1), requires_grad = True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = torch.optim.SGD([W, b], lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n",
      "Epoch    1/20 hypothesis: tensor([67.2578, 80.8397, 79.6523, 86.7394, 61.6605]) Cost: 9298.520508\n",
      "Epoch    2/20 hypothesis: tensor([104.9128, 126.0990, 124.2466, 135.3015,  96.1821]) Cost: 2915.712402\n",
      "Epoch    3/20 hypothesis: tensor([125.9942, 151.4381, 149.2133, 162.4896, 115.5097]) Cost: 915.040649\n",
      "Epoch    4/20 hypothesis: tensor([137.7967, 165.6247, 163.1911, 177.7112, 126.3307]) Cost: 287.936157\n",
      "Epoch    5/20 hypothesis: tensor([144.4044, 173.5674, 171.0168, 186.2332, 132.3891]) Cost: 91.371010\n",
      "Epoch    6/20 hypothesis: tensor([148.1035, 178.0143, 175.3980, 191.0042, 135.7812]) Cost: 29.758249\n",
      "Epoch    7/20 hypothesis: tensor([150.1744, 180.5042, 177.8509, 193.6753, 137.6805]) Cost: 10.445281\n",
      "Epoch    8/20 hypothesis: tensor([151.3336, 181.8983, 179.2240, 195.1707, 138.7440]) Cost: 4.391237\n",
      "Epoch    9/20 hypothesis: tensor([151.9824, 182.6789, 179.9928, 196.0079, 139.3396]) Cost: 2.493121\n",
      "Epoch   10/20 hypothesis: tensor([152.3454, 183.1161, 180.4231, 196.4765, 139.6732]) Cost: 1.897688\n",
      "Epoch   11/20 hypothesis: tensor([152.5485, 183.3609, 180.6640, 196.7389, 139.8602]) Cost: 1.710555\n",
      "Epoch   12/20 hypothesis: tensor([152.6620, 183.4982, 180.7988, 196.8857, 139.9651]) Cost: 1.651412\n",
      "Epoch   13/20 hypothesis: tensor([152.7253, 183.5752, 180.8742, 196.9678, 140.0240]) Cost: 1.632369\n",
      "Epoch   14/20 hypothesis: tensor([152.7606, 183.6184, 180.9164, 197.0138, 140.0571]) Cost: 1.625924\n",
      "Epoch   15/20 hypothesis: tensor([152.7802, 183.6427, 180.9399, 197.0395, 140.0759]) Cost: 1.623420\n",
      "Epoch   16/20 hypothesis: tensor([152.7909, 183.6565, 180.9530, 197.0538, 140.0865]) Cost: 1.622141\n",
      "Epoch   17/20 hypothesis: tensor([152.7968, 183.6643, 180.9603, 197.0618, 140.0927]) Cost: 1.621262\n",
      "Epoch   18/20 hypothesis: tensor([152.7999, 183.6688, 180.9644, 197.0661, 140.0963]) Cost: 1.620501\n",
      "Epoch   19/20 hypothesis: tensor([152.8014, 183.6715, 180.9665, 197.0686, 140.0985]) Cost: 1.619764\n",
      "Epoch   20/20 hypothesis: tensor([152.8020, 183.6731, 180.9677, 197.0699, 140.0999]) Cost: 1.619046\n"
     ]
    }
   ],
   "source": [
    "# Full Code with torch.optim(2)\n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    # H(x) 계산 \n",
    "    hypothesis = x_train.matmul(W) + b # or .mm or @\n",
    "    \n",
    "    # cost 계산 \n",
    "    cost = torch.mean((hypothesis-y_train)**2)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print('Epoch {:4d}/{} hypothesis: {} Cost: {:.6f}'.format(epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item() ))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultivariateLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 1) # 입력차원 3, 출력차원 1\n",
    "        # Hypothesis 계산은 forward()에서!\n",
    "        # Gradient 계산은 PyTorch가 알아서 해준다 backward()\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "model = MultivariateLinearRegressionModel()\n",
    "prediction = model(x_train) \n",
    "\n",
    "# cost 계산 \n",
    "cost = F.mse_loss(prediction, y_train) # torch.nn.functional에서 제공하는 Loss function 사용 / 쉽게 다른 Loss와 교체 가능!(l1_loss, smooth_l1_loss 등)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Hypothesis: tensor([14.1488, 15.9470, 16.3604, 16.0360, 13.4672]), Cost: 24653.501953\n",
      "Epoch    1/20 Hypothesis: tensor([75.4660, 89.6465, 88.9773, 95.1140, 69.6815]), Cost: 7728.563477\n",
      "Epoch    2/20 Hypothesis: tensor([109.7950, 130.9082, 129.6328, 139.3869, 101.1540]), Cost: 2423.493652\n",
      "Epoch    3/20 Hypothesis: tensor([129.0144, 154.0093, 152.3943, 164.1736, 118.7744]), Cost: 760.636597\n",
      "Epoch    4/20 Hypothesis: tensor([139.7745, 166.9429, 165.1375, 178.0508, 128.6396]), Cost: 239.418747\n",
      "Epoch    5/20 Hypothesis: tensor([145.7984, 174.1840, 172.2719, 185.8201, 134.1629]), Cost: 76.043953\n",
      "Epoch    6/20 Hypothesis: tensor([149.1709, 178.2382, 176.2662, 190.1698, 137.2553]), Cost: 24.834116\n",
      "Epoch    7/20 Hypothesis: tensor([151.0588, 180.5081, 178.5023, 192.6050, 138.9868]), Cost: 8.782270\n",
      "Epoch    8/20 Hypothesis: tensor([152.1156, 181.7791, 179.7542, 193.9683, 139.9564]), Cost: 3.750461\n",
      "Epoch    9/20 Hypothesis: tensor([152.7070, 182.4908, 180.4551, 194.7316, 140.4994]), Cost: 2.172829\n",
      "Epoch   10/20 Hypothesis: tensor([153.0380, 182.8893, 180.8474, 195.1589, 140.8035]), Cost: 1.677904\n",
      "Epoch   11/20 Hypothesis: tensor([153.2231, 183.1126, 181.0670, 195.3981, 140.9740]), Cost: 1.522344\n",
      "Epoch   12/20 Hypothesis: tensor([153.3266, 183.2377, 181.1899, 195.5319, 141.0696]), Cost: 1.473181\n",
      "Epoch   13/20 Hypothesis: tensor([153.3843, 183.3079, 181.2586, 195.6069, 141.1232]), Cost: 1.457353\n",
      "Epoch   14/20 Hypothesis: tensor([153.4165, 183.3473, 181.2970, 195.6488, 141.1534]), Cost: 1.452004\n",
      "Epoch   15/20 Hypothesis: tensor([153.4342, 183.3695, 181.3185, 195.6722, 141.1705]), Cost: 1.449886\n",
      "Epoch   16/20 Hypothesis: tensor([153.4440, 183.3821, 181.3304, 195.6853, 141.1802]), Cost: 1.448814\n",
      "Epoch   17/20 Hypothesis: tensor([153.4493, 183.3892, 181.3371, 195.6926, 141.1858]), Cost: 1.448086\n",
      "Epoch   18/20 Hypothesis: tensor([153.4521, 183.3933, 181.3407, 195.6966, 141.1891]), Cost: 1.447429\n",
      "Epoch   19/20 Hypothesis: tensor([153.4535, 183.3958, 181.3427, 195.6989, 141.1911]), Cost: 1.446839\n",
      "Epoch   20/20 Hypothesis: tensor([153.4541, 183.3973, 181.3438, 195.7001, 141.1923]), Cost: 1.446229\n"
     ]
    }
   ],
   "source": [
    "model=MultivariateLinearRegressionModel()\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=1e-5)\n",
    "nb_epochs=20\n",
    "for epoch in range(nb_epochs+1):\n",
    "    Hypothesis=model(x_train)\n",
    "    cost=F.mse_loss(Hypothesis,y_train)\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    print('Epoch {:4d}/{} Hypothesis: {}, Cost: {:.6f}'.format(epoch,nb_epochs,Hypothesis.squeeze().detach(),cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1\ty_pred = tensor([-37.5671, -41.9411, -42.9742, -43.3251, -30.8874])\tLoss = 44864.3984375\n",
      "Epoch : 2\ty_pred = tensor([44.3844, 56.6366, 54.1149, 58.8971, 44.3133])\tLoss = 14449.052734375\n",
      "Epoch : 3\ty_pred = tensor([ 90.8891, 112.5763, 109.2099, 116.9052,  86.9874])\tLoss = 4654.6708984375\n",
      "Epoch : 4\ty_pred = tensor([117.2789, 144.3204, 140.4744, 149.8232, 111.2036])\tLoss = 1500.6734619140625\n",
      "Epoch : 5\ty_pred = tensor([132.2541, 162.3343, 158.2160, 168.5033, 124.9456])\tLoss = 485.01922607421875\n",
      "Epoch : 6\ty_pred = tensor([140.7518, 172.5566, 168.2837, 179.1039, 132.7438])\tLoss = 157.95677185058594\n",
      "Epoch : 7\ty_pred = tensor([145.5739, 178.3575, 173.9967, 185.1196, 137.1691])\tLoss = 52.63556671142578\n",
      "Epoch : 8\ty_pred = tensor([148.3100, 181.6494, 177.2385, 188.5336, 139.6803])\tLoss = 18.719371795654297\n",
      "Epoch : 9\ty_pred = tensor([149.8625, 183.5175, 179.0780, 190.4711, 141.1054])\tLoss = 7.797204494476318\n",
      "Epoch : 10\ty_pred = tensor([150.7433, 184.5776, 180.1218, 191.5708, 141.9141])\tLoss = 4.279603004455566\n",
      "Epoch : 11\ty_pred = tensor([151.2429, 185.1792, 180.7140, 192.1950, 142.3731])\tLoss = 3.1464459896087646\n",
      "Epoch : 12\ty_pred = tensor([151.5262, 185.5206, 181.0499, 192.5494, 142.6336])\tLoss = 2.781149387359619\n",
      "Epoch : 13\ty_pred = tensor([151.6868, 185.7144, 181.2404, 192.7508, 142.7814])\tLoss = 2.66306734085083\n",
      "Epoch : 14\ty_pred = tensor([151.7778, 185.8244, 181.3484, 192.8652, 142.8654])\tLoss = 2.624588966369629\n",
      "Epoch : 15\ty_pred = tensor([151.8292, 185.8869, 181.4096, 192.9304, 142.9130])\tLoss = 2.6117987632751465\n",
      "Epoch : 16\ty_pred = tensor([151.8581, 185.9223, 181.4443, 192.9676, 142.9401])\tLoss = 2.607236385345459\n",
      "Epoch : 17\ty_pred = tensor([151.8744, 185.9425, 181.4638, 192.9888, 142.9555])\tLoss = 2.605360269546509\n",
      "Epoch : 18\ty_pred = tensor([151.8834, 185.9540, 181.4748, 193.0011, 142.9643])\tLoss = 2.604330062866211\n",
      "Epoch : 19\ty_pred = tensor([151.8883, 185.9605, 181.4809, 193.0083, 142.9693])\tLoss = 2.6035757064819336\n",
      "Epoch : 20\ty_pred = tensor([151.8909, 185.9643, 181.4842, 193.0126, 142.9722])\tLoss = 2.6029019355773926\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "x_train = Variable(torch.FloatTensor([[73, 80, 75],\n",
    "                                      [93, 88, 93],\n",
    "                                      [89, 91, 90],\n",
    "                                      [96, 88, 100],\n",
    "                                      [73, 66, 70]]))\n",
    "y_train = Variable(torch.FloatTensor([[152],\n",
    "                                      [185],\n",
    "                                      [180],\n",
    "                                      [196],\n",
    "                                      [142]]))\n",
    "\n",
    "class MultivariateLinearRegressionModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultivariateLinearRegressionModel, self).__init__()\n",
    "        self.linear = torch.nn.Linear(3, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "\n",
    "model = MultivariateLinearRegressionModel()\n",
    "\n",
    "criterion = torch.nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)\n",
    "\n",
    "epochs = 20\n",
    "for epoch in range(1, epochs+1):\n",
    "    y_pred = model(x_train)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch : {epoch}\\ty_pred = {y_pred.squeeze().detach()}\\tLoss = {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qtc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3e34661f62bd8e945851969cf164be387ceff78bd554b7a7643e0a350d74b779"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
