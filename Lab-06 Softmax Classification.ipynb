{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab-06 Softmax Classification\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10ac37fb0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For reproducibility\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.FloatTensor([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0900, 0.2447, 0.6652])\n"
     ]
    }
   ],
   "source": [
    "hypothesis = F.softmax(z, dim=0)\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1664, 0.1871, 0.1737, 0.2695, 0.2033],\n",
      "        [0.2002, 0.1783, 0.2218, 0.1944, 0.2054],\n",
      "        [0.1809, 0.2380, 0.2318, 0.1084, 0.2409]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Cross Entropy -> 두 확률분포가 있을 때, 얼마나 두 확률분포가 비슷한지를 알려주는 수치\n",
    "z = torch.rand(3, 5, requires_grad=True)\n",
    "hypothesis = F.softmax(z, dim=1)\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "y = torch.randint(5, (3,)).long() \n",
    "print(y)\n",
    "\n",
    "# Classes = 5 / samples = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_one_hot = torch.zeros_like(hypothesis)\n",
    "y_one_hot.scatter_(1, y.unsqueeze(1), 1) # _(under bar)는 inplace 연산을 의미함. 첫번째 연산은 dim=1방향, 가운데는 index, 마지막은 삽입하고 싶은 숫자를 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4992, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cost = (y_one_hot * -torch.log(hypothesis)).sum(dim=1).mean()\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7935, -1.6760, -1.7504, -1.3114, -1.5929],\n",
       "        [-1.6086, -1.7244, -1.5062, -1.6381, -1.5826],\n",
       "        [-1.7096, -1.4354, -1.4617, -2.2223, -1.4236]], grad_fn=<LogBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Low level\n",
    "torch.log(F.softmax(z, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7935, -1.6760, -1.7504, -1.3114, -1.5929],\n",
       "        [-1.6086, -1.7244, -1.5062, -1.6381, -1.5826],\n",
       "        [-1.7096, -1.4354, -1.4617, -2.2223, -1.4236]],\n",
       "       grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# High level\n",
    "F.log_softmax(z, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4992, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Low level\n",
    "(y_one_hot * -torch.log(F.softmax(z, dim=1))).sum(dim=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4992, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# High level\n",
    "F.nll_loss(F.log_softmax(z, dim=1), y) # NLL은 Negative Log Likleyhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4992, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# More high level\n",
    "F.cross_entropy(z, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [[1, 2, 1 ,1 ], [2, 1, 3, 2], [3, 1, 3, 4], [ 4, 1, 5, 5], [1, 7, 5, 5], [1, 2, 5, 6], [1, 6, 6, 6], [1, 7, 7, 7]]\n",
    "y_train = [2, 2, 2, 1, 1, 1, 0, 0]\n",
    "x_train = torch.FloatTensor(x_train)\n",
    "y_train = torch.LongTensor(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/10000 Cost: 1.098612\n",
      "Epoch  100/10000 Cost: 0.901535\n",
      "Epoch  200/10000 Cost: 0.839114\n",
      "Epoch  300/10000 Cost: 0.807826\n",
      "Epoch  400/10000 Cost: 0.788472\n",
      "Epoch  500/10000 Cost: 0.774822\n",
      "Epoch  600/10000 Cost: 0.764449\n",
      "Epoch  700/10000 Cost: 0.756191\n",
      "Epoch  800/10000 Cost: 0.749398\n",
      "Epoch  900/10000 Cost: 0.743671\n",
      "Epoch 1000/10000 Cost: 0.738749\n",
      "Epoch 1100/10000 Cost: 0.734453\n",
      "Epoch 1200/10000 Cost: 0.730657\n",
      "Epoch 1300/10000 Cost: 0.727271\n",
      "Epoch 1400/10000 Cost: 0.724223\n",
      "Epoch 1500/10000 Cost: 0.721463\n",
      "Epoch 1600/10000 Cost: 0.718948\n",
      "Epoch 1700/10000 Cost: 0.716644\n",
      "Epoch 1800/10000 Cost: 0.714526\n",
      "Epoch 1900/10000 Cost: 0.712570\n",
      "Epoch 2000/10000 Cost: 0.710759\n",
      "Epoch 2100/10000 Cost: 0.709075\n",
      "Epoch 2200/10000 Cost: 0.707507\n",
      "Epoch 2300/10000 Cost: 0.706043\n",
      "Epoch 2400/10000 Cost: 0.704672\n",
      "Epoch 2500/10000 Cost: 0.703385\n",
      "Epoch 2600/10000 Cost: 0.702177\n",
      "Epoch 2700/10000 Cost: 0.701038\n",
      "Epoch 2800/10000 Cost: 0.699964\n",
      "Epoch 2900/10000 Cost: 0.698950\n",
      "Epoch 3000/10000 Cost: 0.697990\n",
      "Epoch 3100/10000 Cost: 0.697081\n",
      "Epoch 3200/10000 Cost: 0.696218\n",
      "Epoch 3300/10000 Cost: 0.695398\n",
      "Epoch 3400/10000 Cost: 0.694618\n",
      "Epoch 3500/10000 Cost: 0.693876\n",
      "Epoch 3600/10000 Cost: 0.693168\n",
      "Epoch 3700/10000 Cost: 0.692493\n",
      "Epoch 3800/10000 Cost: 0.691848\n",
      "Epoch 3900/10000 Cost: 0.691231\n",
      "Epoch 4000/10000 Cost: 0.690640\n",
      "Epoch 4100/10000 Cost: 0.690074\n",
      "Epoch 4200/10000 Cost: 0.689532\n",
      "Epoch 4300/10000 Cost: 0.689012\n",
      "Epoch 4400/10000 Cost: 0.688512\n",
      "Epoch 4500/10000 Cost: 0.688032\n",
      "Epoch 4600/10000 Cost: 0.687570\n",
      "Epoch 4700/10000 Cost: 0.687126\n",
      "Epoch 4800/10000 Cost: 0.686698\n",
      "Epoch 4900/10000 Cost: 0.686285\n",
      "Epoch 5000/10000 Cost: 0.685888\n",
      "Epoch 5100/10000 Cost: 0.685504\n",
      "Epoch 5200/10000 Cost: 0.685134\n",
      "Epoch 5300/10000 Cost: 0.684776\n",
      "Epoch 5400/10000 Cost: 0.684430\n",
      "Epoch 5500/10000 Cost: 0.684096\n",
      "Epoch 5600/10000 Cost: 0.683772\n",
      "Epoch 5700/10000 Cost: 0.683459\n",
      "Epoch 5800/10000 Cost: 0.683155\n",
      "Epoch 5900/10000 Cost: 0.682862\n",
      "Epoch 6000/10000 Cost: 0.682576\n",
      "Epoch 6100/10000 Cost: 0.682300\n",
      "Epoch 6200/10000 Cost: 0.682032\n",
      "Epoch 6300/10000 Cost: 0.681772\n",
      "Epoch 6400/10000 Cost: 0.681519\n",
      "Epoch 6500/10000 Cost: 0.681273\n",
      "Epoch 6600/10000 Cost: 0.681035\n",
      "Epoch 6700/10000 Cost: 0.680803\n",
      "Epoch 6800/10000 Cost: 0.680577\n",
      "Epoch 6900/10000 Cost: 0.680358\n",
      "Epoch 7000/10000 Cost: 0.680144\n",
      "Epoch 7100/10000 Cost: 0.679936\n",
      "Epoch 7200/10000 Cost: 0.679733\n",
      "Epoch 7300/10000 Cost: 0.679536\n",
      "Epoch 7400/10000 Cost: 0.679344\n",
      "Epoch 7500/10000 Cost: 0.679156\n",
      "Epoch 7600/10000 Cost: 0.678973\n",
      "Epoch 7700/10000 Cost: 0.678795\n",
      "Epoch 7800/10000 Cost: 0.678621\n",
      "Epoch 7900/10000 Cost: 0.678451\n",
      "Epoch 8000/10000 Cost: 0.678285\n",
      "Epoch 8100/10000 Cost: 0.678123\n",
      "Epoch 8200/10000 Cost: 0.677965\n",
      "Epoch 8300/10000 Cost: 0.677810\n",
      "Epoch 8400/10000 Cost: 0.677659\n",
      "Epoch 8500/10000 Cost: 0.677511\n",
      "Epoch 8600/10000 Cost: 0.677367\n",
      "Epoch 8700/10000 Cost: 0.677226\n",
      "Epoch 8800/10000 Cost: 0.677087\n",
      "Epoch 8900/10000 Cost: 0.676952\n",
      "Epoch 9000/10000 Cost: 0.676820\n",
      "Epoch 9100/10000 Cost: 0.676690\n",
      "Epoch 9200/10000 Cost: 0.676563\n",
      "Epoch 9300/10000 Cost: 0.676439\n",
      "Epoch 9400/10000 Cost: 0.676317\n",
      "Epoch 9500/10000 Cost: 0.676198\n",
      "Epoch 9600/10000 Cost: 0.676081\n",
      "Epoch 9700/10000 Cost: 0.675967\n",
      "Epoch 9800/10000 Cost: 0.675855\n",
      "Epoch 9900/10000 Cost: 0.675745\n",
      "Epoch 10000/10000 Cost: 0.675637\n"
     ]
    }
   ],
   "source": [
    "# 모델 초기화\n",
    "W = torch.zeros((4, 3), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "# Optimizer 설정\n",
    "optimizer = optim.SGD([W,b], lr=0.1)\n",
    "\n",
    "nb_epochs = 10000\n",
    "for epoch in range(nb_epochs+1):\n",
    "\n",
    "    # Cost 계산(1)\n",
    "    hypothesis = F.softmax(x_train.matmul(W)+b, dim=1)\n",
    "    y_one_hot = torch.zeros_like(hypothesis)\n",
    "    y_one_hot.scatter_(1, y_train.unsqueeze(1), 1)\n",
    "    cost = (y_one_hot * -torch.log(F.softmax(hypothesis, dim=1))).sum(dim=1).mean()\n",
    "\n",
    "    # Cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(epoch, nb_epochs, cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qtc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3e34661f62bd8e945851969cf164be387ceff78bd554b7a7643e0a350d74b779"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
